{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aist2000/ML-public/blob/master/Bus_Differential.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCv3DQ67PXvE"
      },
      "source": [
        "# Electric BUS\n",
        "[Diagram](https://www.webgreenstation.com/bus-differential-element-ge-ur-multilin-settings-gu8012/)\n",
        "\n",
        "[Compare models](https://ruslanmv.com/blog/The-best-binary-Machine-Learning-Model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DXEJJ7QPZId"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import array as arr\n",
        "from sklearn import datasets, linear_model, metrics\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyerQEzJGOgz"
      },
      "source": [
        "url=\"https://raw.githubusercontent.com/aist2000/ML-public/master/dataset.csv\"\n",
        "#url=\"dataset.csv\"\n",
        "df = pd.read_csv(url, skiprows=0)\n",
        "display(df.head())\n",
        "display(df.describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#histogram to see distribution of data points\n",
        "import seaborn as sns\n",
        "sns.pairplot(data=df, diag_kind='kde', vars=['X1','X2','X3','X4'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vlHl56iqgwH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC-e8G-cC8Pw"
      },
      "source": [
        "X = df[df.columns[0:4]]\n",
        "y = df[['Y']]\n",
        "#X=X.rename(columns={0:'X1',1:'X2', 2:'X3', 3:'X4', 4:'label'})\n",
        "#y = df[['Status']].rename(columns={'Status':'label'})\n",
        "display(X.head())\n",
        "display(y.head())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-14T06:17:32.581431Z",
          "start_time": "2021-06-14T06:17:32.529124Z"
        },
        "id": "1fJFffUPMmcc"
      },
      "outputs": [],
      "source": [
        "# scale features - Not needed\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
        "numeric=['X1','X2','X3','X4']\n",
        "sc=StandardScaler()\n",
        "X_scaled=sc.fit_transform(X)\n",
        "display( \"Scaled\", X_scaled )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset into train and test \n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "#X_train = X\n",
        "#y_train = y\n",
        "display( \"Train\", X_train.describe() )\n",
        "display( \"Test\", X_test.describe() )"
      ],
      "metadata": {
        "id": "IK-xFKWo5k0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XEctKSk4vBgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compare models\n",
        "models = {}\n",
        "\n",
        "# Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "models['Logistic Regression'] = LogisticRegression()\n",
        "\n",
        "# Support Vector Machines\n",
        "from sklearn.svm import LinearSVC\n",
        "models['Support Vector Machines'] = LinearSVC()\n",
        "\n",
        "# Decision Trees\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "models['Decision Trees'] = DecisionTreeClassifier()\n",
        "\n",
        "# Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "models['Random Forest'] = RandomForestClassifier()\n",
        "\n",
        "# Naive Bayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "models['Naive Bayes'] = GaussianNB()\n",
        "\n",
        "# K-Nearest Neighbors\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "models['K-Nearest Neighbor'] = KNeighborsClassifier()\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "accuracy, precision, recall = {}, {}, {}\n",
        "\n",
        "for key in models.keys():\n",
        "    \n",
        "    # Fit the classifier model\n",
        "    models[key].fit(X_train, y_train.values.ravel())\n",
        "    \n",
        "    # Prediction \n",
        "    predictions = models[key].predict(X_test)\n",
        "    \n",
        "    # Calculate Accuracy, Precision and Recall Metrics\n",
        "    accuracy[key] = accuracy_score(predictions, y_test.values.ravel())\n",
        "    precision[key] = precision_score(predictions, y_test.values.ravel())\n",
        "    recall[key] = recall_score(predictions, y_test.values.ravel())"
      ],
      "metadata": {
        "id": "jRBsQDbBo5YH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_model = pd.DataFrame(index=models.keys(), columns=['Accuracy', 'Precision', 'Recall'])\n",
        "df_model['Accuracy'] = accuracy.values()\n",
        "df_model['Precision'] = precision.values()\n",
        "df_model['Recall'] = recall.values()\n",
        "\n",
        "df_model"
      ],
      "metadata": {
        "id": "N-U99yPIpsHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decision Trees**"
      ],
      "metadata": {
        "id": "f9v915hovdoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict Sample with Decision Trees\n",
        "x_t =[[-0.620128,\t0.0\t,0.199953,\t0.130023],[1.870952,\t0.0,\t0.000548,\t-0.730424],[2.0, 0, 2.00, -165] ,[4.5, 0, 0.89, -245]]\n",
        "model = models['Decision Trees'] \n",
        "pred1= model.predict( x_t)\n",
        "#proba1= model.predict_proba(x_t)\n",
        "print(\"For parameters = {}, \\n we predict {} \".format(x_t, pred1))\n",
        "\n",
        "# train accuracy\n",
        "model_accuracy=model.score(X_train,  y_train.Y)\n",
        "y_pred= model.predict( X_train)\n",
        "calc_accuracy =  sum(y_pred == y_train.Y) /  y_train.Y.count()\n",
        "print(\"Train Accuracy Count={}; Model Accuracy= {} ; Calculated Accuracy={}\".format( y_train.Y.count(), model_accuracy, calc_accuracy) )\n",
        "\n",
        "# test accuracy\n",
        "x_t = X_test.head()\n",
        "y_t = y_test.head()\n",
        "#print(X_test.head(), y_test.head())\n",
        "pred1= model.predict( x_t)\n",
        "#proba1= model.predict_proba(x_t)\n",
        "print(\"For parameters = {}{}, \\n we predict {} \".format(x_t, y_t, pred1))\n",
        "\n",
        "model_accuracy=model.score(X_test,  y_test.Y)\n",
        "y_pred= model.predict( X_test)\n",
        "calc_accuracy =  sum(y_pred == y_test.Y) /  y_test.Y.count()\n",
        "print(\"Test Accuracy Count={}; Model Accuracy= {} ; Calculated Accuracy={}\".format( y_test.Y.count(), model_accuracy, calc_accuracy) )\n"
      ],
      "metadata": {
        "id": "o4faDu93p1dU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-Lvz6OvVCAq"
      },
      "source": [
        "**Scilearn Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqSsTc5UXLwZ"
      },
      "source": [
        "#  Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# instantiate the model (using the default parameters)\n",
        "model = LogisticRegression(solver=\"liblinear\", C= 1 )\n",
        "#model = LogisticRegression(max_iter =400, solver='lbfgs')\n",
        "\n",
        "# fit the model with data\n",
        "y_t=y_train.to_numpy().reshape(-1)\n",
        "\n",
        "model.fit(X_train, y_t)\n",
        "\n",
        "display(model.coef_, model.intercept_)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeM5qQfcn4wv"
      },
      "source": [
        "# predict Sample with Logistic Regression\n",
        "#x_t =[[8,0,1.61,-205],[8.0, 0, 1.61, -35],[2.0, 0, 2.00, -165] ,[4.5, 0, 0.89, -245]]\n",
        "\n",
        "# train accuracy\n",
        "model_accuracy=model.score(X_train,  y_train)\n",
        "y_pred= model.predict( X_train)\n",
        "calc_accuracy =  sum(y_pred == y_train.Y) /  y_train.Y.count()\n",
        "print(\"Train  Count={}; Model Accuracy= {} ; Calculated Accuracy={}\".format( y_train.Y.count(), model_accuracy, calc_accuracy) )\n",
        "\n",
        "# test accuracy\n",
        "x_t = X_test.head()\n",
        "y_t = y_test.head()\n",
        "#print(X_test.head(), y_test.head())\n",
        "pred1= model.predict( x_t)\n",
        "#proba1= model.predict_proba( np.array(x_t, ndmin=2) )\n",
        "proba1= model.predict_proba(x_t)\n",
        "print(\"For parameters = {} \\n{}, \\n we predict {} proba={}\".format(x_t, y_t, pred1, proba1[:,1]))\n",
        "\n",
        "model_accuracy=model.score(X_test,  y_test.Y)\n",
        "y_pred= model.predict( X_test)\n",
        "calc_accuracy =  sum(y_pred == y_test.Y) /  y_test.Y.count()\n",
        "print(\"Test  Count={}; Model Accuracy= {} ; Calculated Accuracy={}\".format( y_test.Y.count(), model_accuracy, calc_accuracy) )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-t3zQMAvTJp"
      },
      "source": [
        "**Standard Vector Machine SVM**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard Vector Machine SVM\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "model = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
        "\n",
        "y_t=y_train.to_numpy().reshape(-1)\n",
        "model.fit(X_train, y_t)\n",
        "\n",
        "\n",
        "#print(clf.predict([[-0.8, -1]]))"
      ],
      "metadata": {
        "id": "TrfyKb8h9F5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict Sample with SVM\n",
        "x_t =[[-0.620128,\t0.0\t,0.199953,\t0.130023],[1.870952,\t0.0,\t0.000548,\t-0.730424],[2.0, 0, 2.00, -165] ,[4.5, 0, 0.89, -245]]\n",
        "pred1= model.predict( x_t)\n",
        "#proba1= model.predict_proba(x_t)\n",
        "print(\"For parameters = {}, \\n we predict {} \".format(x_t, pred1))\n",
        "\n",
        "# train accuracy\n",
        "model_accuracy=model.score(X_train,  y_train.Y)\n",
        "y_pred= model.predict( X_train)\n",
        "calc_accuracy =  sum(y_pred == y_train.Y) /  y_train.Y.count()\n",
        "print(\"Train Accuracy Count={}; Model Accuracy= {} ; Calculated Accuracy={}\".format( y_train.Y.count(), model_accuracy, calc_accuracy) )\n",
        "\n",
        "# test accuracy\n",
        "x_t = X_test.head()\n",
        "y_t = y_test.head()\n",
        "#print(X_test.head(), y_test.head())\n",
        "pred1= model.predict( x_t)\n",
        "#proba1= model.predict_proba(x_t)\n",
        "print(\"For parameters = {}{}, \\n we predict {} \".format(x_t, y_t, pred1))\n",
        "\n",
        "model_accuracy=model.score(X_test,  y_test.Y)\n",
        "y_pred= model.predict( X_test)\n",
        "calc_accuracy =  sum(y_pred == y_test.Y) /  y_test.Y.count()\n",
        "print(\"Test Accuracy Count={}; Model Accuracy= {} ; Calculated Accuracy={}\".format( y_test.Y.count(), model_accuracy, calc_accuracy) )\n",
        "#print(X_test[y_pred != y_test.Y].count())\n",
        "print(y_test[y_pred != y_test.Y].head(10))\n",
        "print(y_pred[y_pred != y_test.Y])"
      ],
      "metadata": {
        "id": "_MwNly0F-PmG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}